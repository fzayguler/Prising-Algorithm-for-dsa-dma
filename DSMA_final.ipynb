{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81702e7b-370c-40cb-8ef4-ef03e6599bb4",
   "metadata": {},
   "source": [
    "## DSA/DMA PRICING \n",
    "\n",
    "#### This code generates a price for every article in DSA(Digital Service Act) and DMA(Digital Market Act) regulations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e5b9f-a7b1-411a-b9eb-6b55118d896e",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ef086e-56f2-4216-9e40-827b3289c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "import string\n",
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e3d75c1-9bd9-4291-b1f2-23caa9007a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data):\n",
    "    #cleaning the data, lowering the cases, puctiation, stopwords\n",
    "    # data is a string of text\n",
    "    \n",
    "    tokens = word_tokenize(data)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    #lemmatization with spacy\n",
    "    #nlp = spacy.load('/Users/fzayguler/opt/anaconda3/envs/ghostwriter/lib/python3.9/site-packages/en_core_web_sm/en_core_web_sm-3.3.0')#alternative way of loading spacy\n",
    " \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    data = ' '.join(words)\n",
    "    doc = nlp(data)\n",
    "    cleanwords = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return cleanwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bfdfe-b374-4653-8705-382c4bb08272",
   "metadata": {},
   "source": [
    "### Splitting by Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9d588e-d985-4d8a-b2d5-8f45ee274636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting by pattern by using regex. In the original text Every article stats wirh a '(', a number and ')'. This function splits the text by these patterns.\n",
    "def get_splits(txt):\n",
    "\n",
    "    with open(('/Users/fzayguler/dsa/NLPRegulationsFinal/data/dsa.txt'), 'r') as file:\n",
    "        txt = file.read()\n",
    "\n",
    "    pattern = re.compile(r'\\n?\\([0-9]+\\)\\n')\n",
    "    # res = re.findall(pattern, txt)\n",
    "    splits = re.split(pattern, txt)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3d551-9977-41e5-a0a1-88b7b30753a2",
   "metadata": {},
   "source": [
    "### Google Ads Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7e92cf8-78e3-452b-9bd2-a909545dffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google add prices also known as CPC(click per count) extracted with an api and saved as a json file\n",
    "def get_google_ads_prices(path, features):\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return [{d['name']:d['CPC']} for d in data if d['name'] in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43920124-0510-4c65-9296-574693d12a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search the Google Analytics prices for each word and use a default 10 if the price doesn't exist\n",
    "#I am using only monograms in the code. Monograms sometimes doesnt mean anything for the Google Ads. \n",
    "#The problem was Google Ads had trouble finding one word keywords because it is not common. I will try implementing the code for ngrams in the furure.\n",
    "def parse_prices(GAprices, words):\n",
    "    vals = [tuple(d.items()) for d in GAprices]\n",
    "    gaPrices = []\n",
    "    for w in words:\n",
    "        for d in GAprices:\n",
    "            p = d.get(w,None)\n",
    "        if p:\n",
    "            gaPrices.append(p)\n",
    "        else:\n",
    "            gaPrices.append(10)\n",
    "    return gaPrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a7042-fc85-4267-874f-aa4afcd31de4",
   "metadata": {},
   "source": [
    "### The Flesch–Reading-Ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8938c897-80c9-45aa-9ca0-9fd78fdbffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Flesch–Reading-Ease is a readability test designed to indicate how difficult a passage in English is to understand. https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease\n",
    "#higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. \n",
    "#The scores are generally between 0-100 butthere is no limit on how low the score can be. A negative score is also valid. Since the sentences are quite long My scores was quite low and somethimes even negative.\n",
    "\n",
    "def get_readability_metrics(split):\n",
    "\n",
    "    reading_ease = textstat.flesch_reading_ease(split)\n",
    "    mcalpine = textstat.mcalpine_eflaw(split)\n",
    "    # textstat.reading_time(split, ms_per_char=14.69)\n",
    "    return reading_ease, mcalpine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85305a64-bd98-4cd1-b0a6-034de351a834",
   "metadata": {},
   "source": [
    "### TF/IDF\n",
    "##### Calculate IDF by using every article as document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83cfa28c-f3ba-43dd-883c-15c71d1d8ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d775094f-1cd3-48f0-b120-aaf7f60ab569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IDF(vectorizer:TfidfVectorizer):\n",
    "    vocab = vectorizer.vocabulary_\n",
    "    idf = vectorizer.idf_\n",
    "    #scaling the values to make the difference bigger\n",
    "    return {key:2**(idf[val]) for key, val in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c576836-266b-4def-824c-98680fd19a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just a helper to get an array of IDF values for each of the considered words\n",
    "def get_idf_embedding(article, idf):\n",
    "    words = article.split(\" \")\n",
    "    return np.array([idf.get(w,1) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a3fd9-96a0-482e-8a97-a35276aedaaa",
   "metadata": {},
   "source": [
    "### Pricing Function\n",
    "\n",
    "#### Price of article = sum (GoogleAddPrice (ngram) x if/idf(monogram)) + TextReadibility x cte \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2014b7c0-a6ad-4fab-8e0e-a2361223ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For now I am using GoogleAddPrice monogram but the code will be implemented with ngrams in the future\n",
    "def get_price(cleaned_data, googleAdPrice, vectorizer: TfidfVectorizer):\n",
    "    idf = get_IDF(vectorizer)\n",
    "    prices = []\n",
    "    for c in cleaned_data:\n",
    "        # emb = vectorizer.transform([c]).toarray()\n",
    "        words = c.split(\" \")\n",
    "        emb = get_idf_embedding(c, idf)\n",
    "        # GAprices = np.random.random_integers(low=10, high=1000, size = emb.shape)\n",
    "        GAprices = np.array(parse_prices(googleAdPrice, words))\n",
    "        # print(f\"Embedding Shape, {emb.shape}\")\n",
    "        # print(f\"GAprices Shape, {GAprices.shape}\")\n",
    "        reading_ease = get_readability_metrics(c)\n",
    "        aux = (emb*GAprices).sum()+ 10 * reading_ease\n",
    "        prices.append(aux)\n",
    "    return np.array(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848fb14d-4ed8-4e3c-a232-1cf497fcdbdb",
   "metadata": {},
   "source": [
    "### Making the Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d18e05ef-2add-4278-998b-c77e324f1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Vectorizer\n",
    "#Change the ngram_range=[1, 3] for trigrams\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=[1, 1], use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0f34c3e-c78c-49bb-b004-6dfd7fe2aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document in articles\n",
    "splits = get_splits(\"/Users/fzayguler/dsa/NLPRegulationsFinal/data/dsa.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc18e33b-0be3-4c77-94dc-205454c6314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean it using the clean function\n",
    "cleaned_text = [clean_text(split) for split in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b8d93a1-9200-4df1-972a-820447af024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert nested list into string for if/idf\n",
    "cleaned_data = [' '.join([str(c) for c in lst]) for lst in cleaned_text]\n",
    "# cleaned_text_concat = [j for i in cleaned_text for j in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e2b5df5-f3dc-4731-9853-2f997ed2cfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ability  able  absence  abstain  abuse  abusive    access  accessibility  \\\n",
      "0      0.0   0.0      0.0      0.0    0.0      0.0  0.000000            0.0   \n",
      "1      0.0   0.0      0.0      0.0    0.0      0.0  0.075114            0.0   \n",
      "2      0.0   0.0      0.0      0.0    0.0      0.0  0.074636            0.0   \n",
      "3      0.0   0.0      0.0      0.0    0.0      0.0  0.000000            0.0   \n",
      "4      0.0   0.0      0.0      0.0    0.0      0.0  0.000000            0.0   \n",
      "\n",
      "   accessible  accommodation  ...  withdraw  within   without  woman  work  \\\n",
      "0         0.0            0.0  ...       0.0     0.0  0.000000    0.0   0.0   \n",
      "1         0.0            0.0  ...       0.0     0.0  0.000000    0.0   0.0   \n",
      "2         0.0            0.0  ...       0.0     0.0  0.067689    0.0   0.0   \n",
      "3         0.0            0.0  ...       0.0     0.0  0.000000    0.0   0.0   \n",
      "4         0.0            0.0  ...       0.0     0.0  0.000000    0.0   0.0   \n",
      "\n",
      "   working  would  xenophobic      year  yet  \n",
      "0      0.0    0.0         0.0  0.000000  0.0  \n",
      "1      0.0    0.0         0.0  0.132873  0.0  \n",
      "2      0.0    0.0         0.0  0.000000  0.0  \n",
      "3      0.0    0.0         0.0  0.000000  0.0  \n",
      "4      0.0    0.0         0.0  0.000000  0.0  \n",
      "\n",
      "[5 rows x 1346 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fzayguler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#just to show how tfidf is used in a matrix. \n",
    "tfidf_separate = tfidf_vectorizer.fit_transform(cleaned_data)\n",
    "\n",
    "df_tfidf2 = pd.DataFrame(tfidf_separate.toarray(\n",
    "), columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "print(df_tfidf2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fec7011-811b-4722-b744-d1aacc335f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fzayguler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "word_lst = tfidf_vectorizer.get_feature_names()\n",
    "count_lst = tfidf_separate.toarray().sum(axis=0)\n",
    "\n",
    "vocab_df = pd.DataFrame((zip(word_lst, count_lst)),\n",
    "                        columns=[\"vocab\", \"tfidf_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "75c008d0-6cec-47ba-93c1-a41d6d2e86d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab</th>\n",
       "      <th>tfidf_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ability</td>\n",
       "      <td>0.584717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>able</td>\n",
       "      <td>1.853461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>absence</td>\n",
       "      <td>0.393788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abstain</td>\n",
       "      <td>0.118115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abuse</td>\n",
       "      <td>0.243452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1341</th>\n",
       "      <td>working</td>\n",
       "      <td>0.089843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1342</th>\n",
       "      <td>would</td>\n",
       "      <td>0.183427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343</th>\n",
       "      <td>xenophobic</td>\n",
       "      <td>0.081581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>year</td>\n",
       "      <td>0.514098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>yet</td>\n",
       "      <td>0.261978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1346 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           vocab  tfidf_value\n",
       "0        ability     0.584717\n",
       "1           able     1.853461\n",
       "2        absence     0.393788\n",
       "3        abstain     0.118115\n",
       "4          abuse     0.243452\n",
       "...          ...          ...\n",
       "1341     working     0.089843\n",
       "1342       would     0.183427\n",
       "1343  xenophobic     0.081581\n",
       "1344        year     0.514098\n",
       "1345         yet     0.261978\n",
       "\n",
       "[1346 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0d13744-6806-43ed-8b5f-6d43a2d84b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df.to_csv('tfidf_mono')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a259d-bb1f-44ca-80f7-87066ca6a38c",
   "metadata": {},
   "source": [
    "### Get the google ad prices and filter them with the features we've got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ff3628a-5e97-447b-878d-2820463ec5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fzayguler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Get the google ad prices and filter them with the features we've got\n",
    "GAprices = get_google_ads_prices(\"/Users/fzayguler/dsa/NLPRegulations/data/dict_word_final.json\", set(tfidf_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8656605-1d4d-4dea-854d-36880c0964a7",
   "metadata": {},
   "source": [
    "### Final Price Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6272692-b6ca-4afb-867b-8b7c3705a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fzayguler/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the prices using the get_price functions and print as csv\n",
    "prices = get_price(cleaned_data, GAprices, tfidf_vectorizer)\n",
    "\n",
    "with open(\"data/GAQuery.json\", \"w\") as f:\n",
    "    data = [{\"name\":v} for v in tfidf_vectorizer.get_feature_names()]\n",
    "    json.dump(data, f)\n",
    "\n",
    "articles = [f\"Article {i}\" for i in range(len(prices))]\n",
    "\n",
    "pricedf = pd.DataFrame(zip(articles, prices), columns=[\"Articles\", \"Prices\"])\n",
    "\n",
    "pricedf.to_csv(\"data/prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8b1ba18-3389-4525-81b1-a3e4d29d2dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A plot of article prices\n",
    "_, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(prices)\n",
    "ax.set_xlabel(\"Article Number\")\n",
    "ax.set_ylabel(\"Price\")\n",
    "# ax.set_yscale(\"log\")\n",
    "plt.savefig(\"prices.jpg\")\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
